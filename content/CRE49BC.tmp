\chapter{Methods and Experiments} \label{methods-and-experiments}

\section{Single-Agent Bilateral Negotiation Environment (\gls{sbe})}
In this environment, agent represents the negotiator in negotiation mechanism.

\subsection{Independent Negotiator in NegMAS}
In the environment has just single learnable \gls{drl} negotiator. All RL algorithms with the correct type of action space and observation space can be tested in this specific environment. In the experiment of this thesis, some algorithms, such as DQN, PPO, A2C, from stable-baselines \ref{backgrounds:stable-baselines} are tested in four learning cases:
\begin{itemize}
	\item single issue, acceptance strategy
	\item single issue, offer strategy
	\item multi-issues, acceptance strategy
	\item multi-issues, offer startegy
\end{itemize}

The training logic of some RL algorithms is shown in the figure \ref{fig:dqn}, and the detailed description of the algorithm is shown in the appendix \ref{appendix:dqn}.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.30\textwidth]{./images/dqn.png}
\caption{Training logic of DQN}
\label{fig:dqn}
\end{figure}

\subsection{Experiment} \label{sbe:experiment}
Figure \ref{fig:bilateral-negotiation} depicts a game composed of two negotiators: \textbf{RL negotiator} and \textbf{Opponent negotiator}(AspirationNegotiator). The type of opponent negotiator can be changed in the settings file. All negotiators, which inherit the base abstract negotiator class in \gls{negmas} can be configured in the experiment. AspirationNegotiator is selected as the baseline negotiator in this experiment.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.50\textwidth]{./images/bilateral-negotiation.png}
\caption{Bilateral Negotiation Game in \gls{sbe}, My Deep Reinforcement Learning Negotiator vs. Aspiration Negotiator}
\label{fig:bilateral-negotiation}
\end{figure}

Negotiation mechanism is \gls{saom}, RL negotiator can learn two strategies called \textbf{acceptance strategy} and \textbf{offer strategy}, which will be described in detail in the following paragraphs. 

\paragraph{Acceptance strategy} Actions of agent are \texttt{Accept offer}, \texttt{Wait} and \texttt{Reject offer}. The variables observed by the agent are current offer of opponent and current time(running time, or relative round of negotiation mechanism).

\paragraph{Offer strategy} Actions of negotiator are the set of outcome in negotiation mechanism. The observation space is same as it defined in the acceptance strategy. There is a special requirement when learning this strategy: Before feeding variables into the algorithm, action and observation are normalized.

Two different categories of agents are used for the experiments.

\paragraph{RL agent} was trained in the training environment for acceptance strategy and for offer strategy. Each strategy was learned under single issue(Table \ref{tab:attributes-sbe}) and under multi-issues(Table \ref{tab:attributes-sbe-multi-issues}) cases.

\paragraph{Heuristic agents(e.g., AspirationNegotiator)} Negotiators were implemented in the package scml. In this experiment, the baseline negotiator is \textbf{ApsirationNegotiator} \ref{related-work:heuristic-negotiation}, which is a time-based heuristic negotiator.

\subsubsection{single issue}
The training environment is based on the \gls{sbe} and sets concrete limits and attributes for it, which are defined in table \ref{tab:attributes-sbe}.

\begin{table}[htbp]
\centering
\begin{tabular}{l l l} \toprule
\bfseries \textbf{Attributes}      & \bfseries \textbf{Value}             \\ \midrule
\textbf{Name}                    & negotiation\_env\_ac\_s, negotiation\_env\_of\_s \\
\textbf{Negotiation Mechanism}   & SAOMechanism                                        \\
\textbf{Max\_Steps}              & 100                                                 \\
\textbf{Issue}             	     & Price(300, 550)                                     \\
\textbf{Competitors}             & [MyDRLNegotiator, AspirationNegotiator]             \\
\textbf{Utility Functions}       & [LinearUtility(-0.35), LinearUtility(0.25)]         \\
\textbf{Actions}                 & [ACCEPT, REJCT, WAIT, END], Outcomes\\
\bottomrule
\end{tabular}
\caption{Attributes of the training environment(sbe), single-issue}
\label{tab:attributes-sbe}
\end{table}

Algorithms \gls{dqn}, \gls{ppo}, ACER\parencite{DBLP:journals/corr/WangBHMMKF16}, A2C and DDPG are tested in the cases of single issue. The curve of mean episode reward of case  \textbf{single issue, acceptance strategy} and \textbf{single issue, offer strategy} are shown in \ref{fig:single-issue}. \gls{dqn}, ACER, \gls{ppo} and A2C support the discrete action space. Hence, these algorithms are used for training acceptance strategy, its action space is discrete. Additionally, \gls{ddpg}, \gls{ppo} and A2C can be used for training offer strategy, its action spaces can be considered as continuous. The curve of mean episode reward is represented as two type: 
\begin{itemize}
\item step: combines mean episode reward of all algorithms into one diagram.
\item wall: splits the mean episode reward of all algorithms as horizontal independente diagram.
\end{itemize}

\begin{figure}
    \includegraphics[width=.44\textwidth]{./images/ac_s.png}\hfill
    \includegraphics[width=.44\textwidth]{./images/ac_s_wall.png}
    \\[\smallskipamount]
    \includegraphics[width=.44\textwidth]{./images/of_s.png}\hfill
    \includegraphics[width=.44\textwidth]{./images/of_s_wall.png}
    \caption{Mean reward of \textbf{Acceptance Strategy}(top left(step), top right(wall)) and of \textbf{Offer Strategy}(bottom left(step), bottom right(wall)) under single issue negotiation}
		\label{fig:single-issue}
\end{figure}

\paragraph{Evaluation} When training independent negotiator with SAOMechanism in the environment \gls{sbe}, almost all well-known \gls{drl} algorithms have the ability to learn. \gls{acer}, \gls{ppo} and A2C have learned very good acceptance strategy, the mean reward curve converged to around 70(i.e., 68.9). However, the performance of dqn is not particularly good. For learning offer strategy, PPO2(improved version of \gls{ppo}) and A2C perform best. The reward curve converged to around 100. The result of \gls{ddpg} are also valuable. Although the reward curve does not converge, it oscillates around a better strategy. Overall, normal version \gls{ppo} does not perform well here. 

\subsubsection{multi issues}
The training environment is almost same as the training environment for single issue. It is based on the \gls{sbe} and sets concrete limits and attributes for it, which are defined in table \ref{tab:attributes-sbe-multi-issues}.

\begin{table}[htbp]
\centering
\begin{tabular}{l l l} \toprule
\bfseries \textbf{Attributes}      & \bfseries \textbf{Value}             \\ \midrule
\textbf{Name}                    & negotiation\_env\_ac\_s, negotiation\_env\_of\_s                     \\
\textbf{Negotiation Mechanism}   & SAOMechanism                                                         \\
\textbf{Max\_Steps}              & 100                                                                  \\
\textbf{Issue}             	     & [Quantity(0, 100), Time(0, 100), Price(10, 100)]                     \\
\textbf{Competitors}             & [MyDRLNegotiator, AspirationNegotiator]                              \\
\textbf{Utility Functions}       & [LinearUtility((0, -0.25, -0.6)), LinearUtility((0, 0.25, 1))]       \\
\textbf{Actions}                 & [ACCEPT, REJCT, WAIT, END], Outcomes                                 \\
\bottomrule
\end{tabular}
\caption{Attributes of the training environment(sbe), multi-issues}
\label{tab:attributes-sbe-multi-issues}
\end{table}

As same as under single issue cases, the curve of mean episode reward of case  \textbf{multi-issues, acceptance strategy} and \textbf{multi -issues, offer strategy} are shown in \ref{fig:multi-issues}. 
\begin{figure}
    \includegraphics[width=.40\textwidth]{./images/ac_s_multi_issues.png}\hfill
    \includegraphics[width=.40\textwidth]{./images/ac_s_multi_issues_wall.png}
    \\[\smallskipamount]
    \includegraphics[width=.40\textwidth]{./images/of_s_multi_issues.png}\hfill
    \includegraphics[width=.40\textwidth]{./images/of_s_multi_issues_wall.png}
    \caption{Mean reward of \textbf{Acceptance Strategy}(top left(step), top right(wall)) and of \textbf{Offer Strategy}(bottom left(step), bottom right(wall)) under multi-issues negotiation}
		\label{fig:multi-issues}
\end{figure}

\paragraph{Evaluation} The characteristics of the mean episode reward curve are the same as in single-issue cases. After increasing the action space, the training time increased rapidly. 

\section{Multi-Agent Concurrent Bilateral Negotiation Environment (\gls{mcbe})}
In this environment, agent represents the factory manager and negotiation controller in standard \gls{scml} and \gls{scml} OneShot, respectively.

The agent interacting with environment may be have many related trainable agents(e.g. one seller, one buyer, named as trainer) as the part of learner in the model. Each seller and buyer controls multiple negotiation sessions. The detail of interactive logic is shown below in \ref{fig:interacting-logic-scml}

\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\textwidth]{./images/scnk.png}
\caption{Interactive logic based on the perspective of \gls{scml}. N: The maximum number of concurrent negotiations for a single agent}
\label{fig:interacting-logic-scml}
\end{figure}

Where environment contains six factories and two system entities(SELLER and BUYER). Three RL agents(A1, A2, An) are located at two production positions. Each RL agent contains three parts:

\begin{itemize}
\item \textbf{Perceptor} Receives state, reward from environment and send these to Learner.
\item \textbf{Actuator} Receives action from Learner and execute it in the environment.
\item \textbf{Learner} In addition to connecting with \textbf{Perceptor} and \textbf{Actuator}, it manages the multiple trainers.
\end{itemize}

Based on different algorithms, the internal components of the trainer are different. In general, the trainer will handle the training logic of concurrent negotiation. It will be introduced in the specific algorithm, diagramed in figures \ref{fig:method-maddpg-scml} and \ref{fig:method-qmix-scml}.

\subsection{\gls{maddpg} in \gls{scml}} \label{methods:maddpg}
In the standard scml environment, two questions are tried to be fixed with maddpg. 

\paragraph{Question 1: Dynamic Range Of Negotiation Issues}At the beginning of every negotiation in simulator, agent will determine the range which constraints value interval for negotiation issues. In the experiment, the negotiation issues are \textbf{QUANTITY}, \textbf{PRICE} and \textbf{TIME}. After creating the simulation world, simulator determines the minimum and maximum values for each negotiation issue taken by the entire simulation episode, such as value of \textbf{QUANTITY} between (1, 10), \textbf{PRICE} between (0, 100) and \textbf{TIME} between (0, 100). However, for every negotiation mechanism created inside the entire simulation episode, it has its dynamic range of negotiation issues which affect the negotiation process. This question was raised based on such a situation.

\paragraph{Question 2: The Offer For Every Round}From the description of question 1, we can find, action obtained by algorithm influences only finite the state of environment. Agent(Factory Manager) can not control the function \textbf{proposal} of every negotiation round. Every negotiation round has always been controlled by heuristic negotiation strategy. Intuitively, the main influence comes from the sequence action of each round of the negotiation. Hence, question 2 \textbf{The Offer For Every Round} is proposed naturally. After the basic problem is determined, how to design becomes the current problem. 

From an algorithm perspective, the data flow of the model is shown in \ref{fig:method-maddpg-scml}. 
\gls{maddpg} used in \gls{scml}, one trainable agent(trainer) defined in \gls{maddpg} is not equal to the agent defined in \gls{scml}.
It create $D$ process for action exploration, in this environment, \textbf{Dynamical Range Of Negotiation Issues}(Question 1) and \textbf{The Offer For Every Round}(Question 2) are needed to be explored.
The basic concepts of \gls{maddpg} are introduced in chapter background \ref{background:maddpg}. 

\begin{figure}[htbp]
\centering
\includegraphics[width=1.00\textwidth]{./images/scml-maddpg.png}
\caption{\gls{maddpg} used in \gls{mcbe}}
\label{fig:method-maddpg-scml}
\end{figure}

In the model \ref{fig:method-maddpg-scml}, policy output action as input to related agent interacting with the environment, which outputs the observation and reward as the inputs to related trainer. Two trainers are created in the experiment:
\begin{itemize}
\item \textbf{trainer\_seller} controls all sale negotiations, the size of the action space has a linear relationship with the size of the largest concurrent sale negotiations.
\item \textbf{trainer\_buyer} controls all buy negotiations, the size of the action space has a linear relationship with the size of the largest concurrent buy negotiations.
\end{itemize}

Details of the algorithm are described in the appendix \ref{appendix:algorithms:maddpg}.  


\subsection{\gls{qmix} in \gls{scml-oneshot}} \label{methods:qmix-scml-oneshot}

The world created by \gls{scml-oneshot} is described in detail in the chapter background \ref{background-scml}. 

\paragraph{Question: The Offer For Every Step} Unlike in standard scml \textbf{Dynamical Range of Negotiation Issues} is controlled by agents, the system takes over the related control and access authority in scml oneshot. Hence, question 1 in the standard library does not need to be discussed here. Although the design of oneshot world is very different with the standard library, the key question is also how to find the optimal sequence action(offer for every negotiation round).

In the current version \gls{qmix}, which is used in the experiment, one trainable agent is related to one negotiation session. When the agents are located in different locations in the scml world, the agents have different concurrent negotiation maximums. Since the agent \textbf{A1} shown in Figure \ref{fig:interacting-logic-scml} has three consumers, the maximum value of concurrent negotiations of the agent \textbf{A1} is 3. Based on this value, we need to create three trainable agents(trainers) in algorithm, and each trainable agent(trainer) controls one negotiation session of interactive agent.

Data flow is shown in \ref{fig:method-qmix-scml}, the total number of trainers is equal to the sum of the maximum number of simultaneous negotiations of all agents. Additionally, unlike in maddpg, in qmix, there is only one global learner, which can control all trainers together.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.80\textwidth]{./images/scml-qmix.png}
\caption{\gls{qmix} used in \gls{mcbe}}
\label{fig:method-qmix-scml}
\end{figure}

\subsection{Experiment}
In this section the experiments of the concurrent negotiations within the scml simulated world and their results are presented and discussed. It contains mainly two sub-sections: concurrent negotiations in standard scml world and in scml-oneshot world. For each sub-section, first, the training scenario is defined and how the various agents play is described. Second, the configuration of training environment is introuduced in the setting table. At the end of each experiment, the results(i.e., scores or mean episode award) will be evaluated and compared.

\subsubsection{Concurrent Neogtiations in standard \gls{scml}}
Standard \gls{scml} is a complex simulation world, which contains various parts with specific functions. The brief description of this simulation is introduced in chapter Background \ref{background-scml}. The experiment of this thesis focus on only the Negotiation Manager(Negotiation Control Strategy) of Decision-Maker Agent. The above mentioned method maddpg \ref{methods:maddpg} is used in this experiment. 
Scenario is diagrammed in Figure \ref{fig:scenario-standard-scml}.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.80\textwidth]{./images/scenario-standard-scml.png}
\caption{M* represent My Component Based Agent with learner \gls{maddpg}, C* represent Opponent Agents, such as \gls{ind-dec-agent}}
\label{fig:scenario-standard-scml}
\end{figure}

Two different categories of agents are used for the experiments:
\paragraph{RL agents} were trained with 25000 time episodes in the training environment for the algorithms maddpg. The Rl-Agents are titled as M* in the scenario image \ref{fig:scenario-standard-scml}. All RL agents have been trained together. After training, they reload the strategy from disk and run it in the standard scml world.

\paragraph{Heuristic agent(e.g., IndDecentralizingAgent)} acts according to the strategy implemented in the scml package. The heuristic agent is named as C* in the scenario image \ref{fig:scenario-standard-scml}. There are many heuristic agents were developed. In the future work, these agents can be tested and compared.


The training environment is based on the \gls{mcbe} and sets concrete limits and attributes for it, which are defined in table \ref{tab:attributes-mcbe-dynamical-range-negotiation-issue}. The simulated world is SCML2020World designed in the package standard scml. Negotiation mechanism SAOMechanism is the default mechanism of simulated world. The environment sets the negotiation speed(negotiation rate as 1) equal to the world speed. Because the negotiation speed has a self-running speed independent of the world speed. In the default world settings, the same negotiation partner can conduct concurrent negotiations. In this experiment, in order to fix the size of the action space, this is not allowed, which means that the maximum number of concurrent negotiations is the prior knowledge. RL agents can determine the better range of negotiation issues(i.e, actions of agents) by training. Negotiation issues are the quantity, time, and price commonly used in the SCM world.

\begin{table}[htbp]
\centering
\begin{tabular}{l l l} \toprule
\bfseries \textbf{Attributes}    & \bfseries \textbf{Value}                                             \\ \midrule
\textbf{Name}                    & scml                                                                 \\
\textbf{World}                   & SCML2020World                                                        \\
\textbf{Neogitation Mechanism}   & SAOMechanism                                                         \\
\textbf{Max Negotiation Steps}   & 10                                                                   \\
\textbf{Max Simulation Steps}    & 10                                                                   \\
\textbf{Issue}             	     & [Quantity(0, 100), Time(0, 100), Price(10, 100)]                     \\
\textbf{Competitors}             & [MyBasedAgent, DecentralizingAgent]                                  \\
\textbf{Negotiated Rate}         &    1                                                                 \\
\textbf{DNSP}                    &    True                             \\
\textbf{Actions}                 & Range of Negotiation Issues                                          \\
\bottomrule
\end{tabular}
\caption{Attributes of the training environment(mcbe), DNSP: Disallow Negotiation with Same Partners, standard scml}
\label{tab:attributes-mcbe-dynamical-range-negotiation-issue}
\end{table}

The following paragraphs will evaulate the results based on the two questions and method maddpg. 

\paragraph{Evaluation of Question 1:} The results after training are shown in figure \ref{fig:dynamical-range-issues-maddpg}. From the mean episode reward curve(right in the figure \ref{fig:dynamical-range-issues-maddpg}), we can know that the agent has not learned a valuable strategy in this situation. The mean episode reward always oscillates around 7. In a sense, this is the best strategy that RL agent can obtain. For this type of situation,  only range of negotiation issues can be controlled by the RL agents, but others parts, such as offer strategy is a normal heuristic strategy. As a result, the RL agents can perform better than random agent, but can not improve the strategy more.

It means, merely changing the dynamic range of negotiation issues cannot effectively improve strategy of RL agents. Compared with baseline agent IndDecentralizingAgent(left in figure \ref{fig:dynamical-range-issues-maddpg}), the score of MyComponentsBasedAgent is not obvious better.  

\paragraph{Evaluation of Question 2:}  
 
Overall, idea from maddpg is very constructive, but it is difficult to learn good strategies with maddpg. Training consumes a lot of time and hardware resources.


\begin{figure}
    \includegraphics[width=.45\textwidth]{./images/dynamic_range_issues_maddpg.png}\hfill
    \includegraphics[width=.49\textwidth]{./images/dynamical_mean_episode_reward.png}
    \caption{Scores(left) of agents running in simulation world after training, Mean Episode Reward(right)}
		\label{fig:dynamical-range-issues-maddpg}
\end{figure}

\subsubsection{Concurrent Negotiations in OneShot \gls{scml}}

World created in scml-oneshot is a new simpler world which only cares about concurrent negotiation in supply chain management, and the agents used in this world can be easily transferred to standard scml. In other words, agent focus only the negotiation control strategy, which is one strategy of three strategies mentioned in section \ref{background:scml2020}.
 
The brief description of this simulated world is introduced in chapter Background \ref{background-scml}. This part of the experiment only focuses on negotiation. The above mentioned method qmix \ref{methods:qmix-scml-oneshot} is used in this experiment. Two scenarios are created: \textbf{self-play} and \textbf{play-with-others}.

\paragraph{self-play}
Scenario is diagrammed in Figure \ref{fig:scenario-oneshot-scml-self-play}.
\begin{figure}[htbp]
\centering
\includegraphics[width=0.80\textwidth]{./images/scenario-oneshot-scml-self-play.png}
\caption{M* represent My Component Based Agent with learner \gls{qmix}}
\label{fig:scenario-oneshot-scml-self-play}
\end{figure}

In this scenario, just only one category of agent, which is RL agent. Because it is the self-play scenario, no other heuristic agents join in.
\paragraph{RL agents} were trained with 10000 time steps in the training environment for algorithm qmix. The RL-agents are named as M* in the scenario image \ref{fig:scenario-oneshot-scml-self-play}. The reward is sum of reward of each agent. Because it is a cooperative game, RL agents try to maximize profit at the end of simulation.

Although the attributes of the scml-oneshot world are very different from the standard scml world, we only consider the negotiation part. Therefore, the negotiation control attributes are almost not changed, such as negotiation mechanism, max negotiation steps, max simulation steps, issues, etc.
The mainly different is we do not need to set the negotiation rate, because all negotiations will be finished inside each world(simulation) step. The name of simulated world is SCML2020OneShotWorld. The actions of agent is joint outcomes. All attributes are defined in table \ref{tab:attributes-mcbe-concurrent-negotiation-scml-oneshot}.

\begin{table}[htbp]
\centering
\begin{tabular}{l l l} \toprule
\bfseries \textbf{Attributes}    & \bfseries \textbf{Value}                                             \\ \midrule
\textbf{Name}                    & scml-oneshot-concurrent-negotiation                                  \\
\textbf{World}                   & SCML2020OneShotWorld                                                 \\
\textbf{Neogitation Mechanism}   & SAOMechanism                                                         \\
\textbf{Max Negotiation Steps}   & 20                                                                  \\
\textbf{Max Simulation Steps}    & 100                                                                   \\
\textbf{Issue}             	     & [Quantity(0, 100), Time(0, 100), Price(10, 100)]                     \\
\textbf{Competitors}             & [MyAgent, MyAgent] (self-play)                                       \\
\textbf{Actions}                 & Joint-Outcomes                                                             \\
\bottomrule
\end{tabular}
\caption{Attributes of the training environment(mcbe), scml-oneshot, self-play}
\label{tab:attributes-mcbe-concurrent-negotiation-scml-oneshot}
\end{table}

\paragraph{play with other agent} In this scenario, the opponent agent is a heuristic agent from the scml packages, such as GreedyAgent.
Scenario is diagrammed in Figure \ref{fig:scenario-oneshot-scml-play-with-greedy}.
\begin{figure}[htbp]
\centering
\includegraphics[width=0.80\textwidth]{./images/scenario-oneshot-scml-play-with-greedy.png}
\caption{M* represent My Component Based Agent with learner \gls{qmix}, G* represent GreedyOneShotAgent}
\label{fig:scenario-oneshot-scml-play-with-greedy}
\end{figure}

Two different categories of agents are used for the experiments:

\paragraph{RL agents} The key parts is same as RL agents mentioned in self-play. In addition to the normal negotiating ability, the agent can also determine whether the negotiator is his teammate. RL agents were trained with 10 thousands time steps in the training environment for algorithm qmix. The RL-agents are named as M* in the scenario image \ref{fig:scenario-oneshot-scml-play-with-greedy}.
\paragraph{Baseline agents} There are many baseline agents realized in scml-oneshot package, such as GreedyAgent, which uses the greedy strategy to act the negotiation action. Overall, it is a simple strategy. In the future, in order to evaluate the performance of this algorithm, more sophisticated agents will need to be tested in this environment. 

The setting of this scenario is same as in the scenario self-play. Only the competitors are changed as MyAgent vs. GreedyAgent. The negotiation mechanism is an alternative rubinstein negotiation mechanism. Negotiation issues are Quantity and Price. Based on the characteristic(all negotiations finished in each world step) of the world, time is not necessary to be considered here(i.e., set the issue time equal to the simulation step in the offer). This characteristic is briefly introduced in the section \ref{background:scml2020-oneshot}. 
All parameters are defined in table \ref{tab:attributes-mcbe-concurrent-negotiation-scml-oneshot-with-others}.

\begin{table}[htbp]
\centering
\begin{tabular}{l l l} \toprule
\bfseries \textbf{Attributes}    & \bfseries \textbf{Value}                                             \\ \midrule
\textbf{Name}                    & scml-oneshot-concurrent-negotiation                                  \\
\textbf{World}                   & SCML2020OneShotWorld                                                 \\
\textbf{Neogitation Mechanism}   & Alternative Rubinstein Mechanism                                                         \\
\textbf{Max Negotiation Steps}   & 20                                                                  \\
\textbf{Max Simulation Steps}    & 100                                                                   \\
\textbf{Issue}             	     & [Quantity(0, 100), Price(10, 100)]                     \\
\textbf{Competitors}             & [MyAgent, GreedyAgent] (play-with-others)                                       \\
\textbf{Actions}                 & Joint-Outcomes                                                             \\
\bottomrule
\end{tabular}
\caption{Attributes of the training environment(mcbe), scml-oneshot, play-with-others}
\label{tab:attributes-mcbe-concurrent-negotiation-scml-oneshot-with-others}
\end{table}

In the following paragraphs the results will be evaulate  based on the two scenarios \textbf{self-paly},\textbf{ play-with-others} and method qmix raised in section \ref{methods:qmix-scml-oneshot}.

\paragraph{Evaluation of self-play} 
Episode mean reward curve is shown in \ref{fig:oneshot-self-play}. After 5000 time steps, the curve converges around 0. In the self-play scenario, the definition of reward is the sum reward of each agent. Therefore, one agent will make more profits, and the other agent will lose more. The reward will eventually converge to a certain value, and it can be seen that the value is close to zero. The explanation of this value will be discussed in the future work. The most important question is whether this state is pareto optimal. The second question is whether RL agents can evolve on their own when the reward function is designed according to the competitive situation. This is very useful for training general RL agents. It means that the agent can learn more unpredictable excellent strategies through self-game.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.55\textwidth]{./images/oneshot_self_play.png}
\caption{Episode mean reward of self paly under SCML OneShot}
\label{fig:oneshot-self-play}
\end{figure}

\paragraph{Evaluation of play-with-others}

Episode mean reward curve is shown in \ref{fig:oneshot-my-vs-greedy}. When the reward is only minus 400 at the beginning, the reward curve converges to around 300. Obviously, RL agents have learned a good strategy. In addition, the training time is also very short. The results are very meaningful.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.55\textwidth]{./images/oneshot_my_vs_greedy.png}
\caption{Episode mean reward of my agent vs GreedyOneShotAgent under SCML OneShot}
\label{fig:oneshot-my-vs-greedy}
\end{figure}

In the scenario \textbf{self-play} and \textbf{play with other agent}, agents learned better strategy than random. It means method \gls{qmix} is valid in scml-oneshot world.

\section{Conclusion}
The training effect of single agent(negotiator) is not bad. But for multi-agent concurrent negotiations it is not easy to implemente. The work of this thesis focus on just two algortims \gls{maddpg} and \gls{qmix}. The performance and results of \gls{qmix} have certain reference value. In the future many work and algorithms are needed to be finished and implemented in the environment \gls{mcbe}.