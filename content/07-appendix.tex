\begin{appendices}
In order to keep clean, some derivation processes were moved to the appendix. Nevertheless, for completeness, detailed algorithms training pseudo-code were provided after the derivation.
	\chapter{Derivation Process} \label{appendices-derivation-process}
	\section{Proof of Policy Gradient Theorem} \label{derivation-process-gradient-pg}
	The policy is usually modeled with a parameterized function respect to $\theta$, $\pi_{\theta}(a|s)$. The value of the reward(objective) function depends on this policy. The reward function is defined as:
	\begin{equation}
	J(\theta)=\sum_{s \in \mathcal{S}} d^{\pi}(s) V^{\pi}(s)=\sum_{s \in \mathcal{S}} d^{\pi}(s) \sum_{a \in \mathcal{A}} \pi_{\theta}(a \mid s) Q^{\pi}(s, a)
\end{equation}
	
	We first start with the derivative of the state value function:
	\begin{equation}
	\begin{aligned}
	 &\nabla_{\theta} V^{\pi}(s) \\
	=&\nabla_{\theta}(\sum_{a \in \mathcal{A}} \pi_{\theta}(a \mid s) Q^{\pi}(s,a)) \\
	=&\sum_{a \in \mathcal{A}}\left(\nabla_{\theta} \pi_{\theta}(a \mid s) Q^{\pi}(s, a)+\pi_{\theta}(a \mid s) \nabla_{\theta} Q^{\pi}(s, a)\right) \\
	=&\sum_{a \in \mathcal{A}}(\nabla_{\theta} \pi_{\theta}(a \mid s) Q^{\pi}(s, a)+\pi_{\theta}(a \mid s) \nabla_{\theta} \sum_{s^{\prime}, r} P\left(s^{\prime}, r \mid s, a\right)\left(r+V^{\pi}\left(s^{\prime}\right)\right)) \\
	=& \sum_{a \in \mathcal{A}}(\nabla_{\theta} \pi_{\theta}(a \mid s) Q^{\pi}(s, a)+\pi_{\theta}(a \mid s) \sum_{s^{\prime}} P\left(s^{\prime} \mid s, a\right) \nabla_{\theta} V^{\pi}\left(s^{\prime}\right)) \\
	=& \phi(s)+\sum_{s^{\prime}} \sum_{a} \pi_{\theta}(a \mid s) P\left(s^{\prime} \mid s, a\right) \nabla_{\theta} V^{\pi}\left(s^{\prime}\right) \\
	=& \phi(s)+\sum \rho^{\pi}\left(s \rightarrow s^{\prime}, 1\right) \nabla_{\theta} V^{\pi}\left(s^{\prime}\right) \\
	=& \phi(s)+\sum \rho^{\pi}\left(s \rightarrow s^{\prime}, 1\right)\left[\phi\left(s^{\prime}\right)+\sum \rho^{\pi}\left(s^{\prime} \rightarrow s^{\prime \prime}, 1\right) \nabla_{\theta} V^{\pi}\left(s^{\prime \prime}\right)\right] \\
	=& \phi(s)+\sum_{1} \rho^{\pi}\left(s \rightarrow s^{\prime}, 1\right) \phi\left(s^{\prime}\right)+\sum_{1 \prime} \rho^{\pi}\left(s \rightarrow s^{\prime \prime}, 2\right) \nabla_{\theta} V^{\pi}\left(s^{\prime \prime}\right) \\
	=& \text{- .; Repeatedly unrolling the part of} \nabla_{\theta} V^{\pi}(.) \\
	=& \sum_{x \in \mathcal{S}} \sum_{k=0}^{\infty} \rho^{\pi}(s \rightarrow x, k) \phi(x)
	\end{aligned}
	\end{equation}
	
	By plugging above into the objective function $J(\theta)$, we are getting the following:
	
	\begin{equation}
	\begin{aligned}
	\nabla_{\theta} J(\theta)=&\nabla_{\theta} V^{\pi}\left(s_{0}\right) \\
	=& \sum_{s} \sum_{k=0}^{\infty} \rho^{\pi}\left(s_{0} \rightarrow s, k\right) \phi(s) \\
	=& \sum_{s} \eta(s) \phi(s) \\
	=& \left(\sum_{s} \eta(s)\right) \sum_{s} \frac{\eta(s)}{\sum_{s} \eta(s)} \phi(s) \\
	\propto&  \sum_{s} \frac{\eta(s)}{\sum_{s} \eta(s)} \phi(s) \\
	=& \sum_{s} d^{\pi}(s) \sum_{a} \nabla_{\theta} \pi_{\theta}(a \mid s) Q^{\pi}(s, a) \\
	=& \sum_{s \in \mathcal{S}} d^{\pi}(s) \sum_{a \in \mathcal{A}} \pi_{\theta}(a \mid s) Q^{\pi}(s, a) \frac{\nabla_{\theta} \pi_{\theta}(a \mid s)}{\pi_{\theta}(a \mid s)} \\
	=& \mathbb{E}_{s \sim d^{\pi}, a \sim \pi_{\theta}}\left[Q^{\pi}(s, a) \nabla_{\theta} \log \pi_{\theta}(a \mid s)\right]
	\end{aligned}
	\end{equation}
	
	Where $d^{\pi}(s)=\frac{\eta(s)}{\sum_{s} \eta(s)}$ is stationary distribution. $\sum_{s} \eta(s)$ is a constant. 
	
	\section{Loss Function of \gls{ppo}} \label{appendix-derivation-process-loss-ppo}
	
  \chapter{Algorithms} \label{appendices-algorithms}
  \section{Single-Agent Reinforcement Learning}
	\subsection{DQN} \label{appendix:dqn}
	For completeness, the DQN algorithm used in Bilateral Negotiation Mechanism from \gls{negmas} is provided below.
	
	\begin{algorithm}[H]
	\SetAlgoLined
	\KwData{}
	\KwResult{}
	Initialize replay buffer $D$ to capacity $N$\;
	Initialize action-value function $Q$ with random weights $\theta$\;
	Initialize target action-value function $\hat{Q}$ with weight $\theta^{-}=\theta$\;
	\For{episode = 1, M}{
	Receive state from simulator $s_1 = \{x_1\}$ and preprocessed state $\phi_{1}=\phi\left(s_{1}\right)$\;
		\For{t = 1, T}{
			With probability $\omega$ select a random action $a_t$ (first step)\;
			otherwise select $a_{t}=\operatorname{argmax}_{a} Q\left(\phi\left(s_{t}\right), a ; \theta\right)$\;
			Execute action $a_t$ in siumulator and observe reward $r_t$ and new state $x_{t+1}$\;		
			Set $s_{t+1} = s_t, a_t, x_{t+1}$ and preprocess $\phi_{t+1} = \phi(s_{t+1})$ \;
			Store transitions $(\phi_j, a_j, r_j, \phi_{t+1})$ in $D$ \;
			Sample random minibatch of transitions $(\phi_j, a_j, r_j, \phi_{j+1})$ from $D$\;
			Set $y_{j}=\left\{\begin{array}{cc}r_{j} & \text { if episode terminates at step } \mathrm{j}+1 \\ r_{j}+\gamma \max _{a^{\prime}} \hat{Q}\left(\phi_{j+1}, a^{\prime} ; \theta^{-}\right) & \text {otherwise }\end{array}\right.$\;
			Perform a gradient descent step on $\left(y_{j}-Q\left(\phi_{j}, a_{j} ; \theta\right)\right)^{2}$ with respect to the network parameters\;
			Every $C$ steps reset $\hat{Q} = Q$\;
		}
	}
	\caption{Deep Q-learning with experience replay}
	\end{algorithm}
	\subsection{PPO} \label{appendix-algorithm-ppo}
	\subsection{DPG}
	\subsection{DDPG}
  \section{Multi-Agent Reinforcement Learning}
	\subsection{\gls{maddpg}} \label{appendix:algorithms:maddpg}
	For completeness, the MADDPG algorithm used in \gls{scml} is provided below.
	
	\begin{algorithm}[H]
  \SetAlgoLined
  \KwData{State comes from simulator \gls{scml}}
  \KwResult{action sequence, proposal offer or set dynamical range of negotiation issues}
	
	\For{episode = 1 to M}{
		Initialize a random process $\mathcal{N}$ for action exploration\;
		Receive the intial state from the Simulator\;
		\For{t = 1 to max-episode-length}{
				for each agent $i$, select action $a_{i}=\boldsymbol{\mu}_{\theta_{i}}\left(o_{i}\right)+\mathcal{N}_{t}$ w.r.t. the current policy and exploration.\;
				Execute joint actions $a=\left(a_{1}, \ldots, a_{N}\right)$ and get the reward $r$ and new state $\mathbf{s}^{\prime}$\;
				Store $\left(\mathbf{s}, a, r, \mathbf{s}^{\prime}\right)$ in replay buffer $\mathcal{D}$\;
				$\mathbf{s} \leftarrow \mathbf{s}^{\prime}$\;
				\For{agent $i=1$ to $N$}{
					Sample a random minibatch of samples $\mathcal{B}$ $\left(\mathbf{s}^{j}, a^{j}, r^{j}, \mathbf{s}^{\prime j}\right)$ from $\mathcal{D}$\;
					Set $y^{j}=r_{i}^{j}+\left.\gamma Q_{i}^{\mu^{\prime}}\left(\mathbf{s}^{\prime j}, a_{1}^{\prime}, \ldots, a_{N}^{\prime}\right)\right|_{a_{k}^{\prime}=\boldsymbol{\mu}_{k}^{\prime}\left(o_{k}^{j}\right)}$ \;
					Update critic by minimizing the loss $\mathcal{L}\left(\theta_{i}\right)=\frac{1}{B} \sum_{j}\left(y^{j}-Q_{i}^{\boldsymbol{\mu}}\left(\mathbf{s}^{j}, a_{1}^{j}, \ldots, a_{N}^{j}\right)\right)^{2}$ \;
					Update actor using the sampled policy gradient:
					\begin{equation}
						\left.\nabla_{\theta_{i}} J \approx \frac{1}{B} \sum_{j} \nabla_{\theta_{i}} \boldsymbol{\mu}_{i}\left(o_{i}^{j}\right) \nabla_{a_{i}} Q_{i}^{\mu}\left(\mathbf{s}^{j}, a_{1}^{j}, \ldots, a_{i}, \ldots, a_{N}^{j}\right)\right|_{a_{i}=\boldsymbol{\mu}_{i}\left(o_{i}^{j}\right)}
					\end{equation}
				}
				Update target network parameters for each agent i: $\theta_{i}^{\prime} \leftarrow \tau \theta_{i}+(1-\tau) \theta_{i}^{\prime}$
			}
	}
  \caption{Multi-Agent Deep Deterministic Policy Gradient for N agents}
\end{algorithm}

	\subsection{\gls{qmix}}
	
\end{appendices}