\begin{appendices}
In order to keep clean, some derivation processes were moved to the appendix. Nevertheless, for completeness, detailed algorithms training pseudo-code were provided after the derivation.
	\chapter{Derivation Process} \label{appendices-derivation-process}
	\section{Proof of Policy Gradient Theorem} \label{derivation-process-gradient-pg}
	The policy is usually modeled with a parameterized function respect to $\theta$, $\pi_{\theta}(a|s)$. The value of the reward(objective) function depends on this policy. The reward function is defined as:
	\begin{equation}
	J(\theta)=\sum_{s \in \mathcal{S}} d^{\pi}(s) V^{\pi}(s)=\sum_{s \in \mathcal{S}} d^{\pi}(s) \sum_{a \in \mathcal{A}} \pi_{\theta}(a \mid s) Q^{\pi}(s, a)
\end{equation}

	We first start with the derivative of the state value function:
	\begin{equation}
	\begin{aligned}
	 &\nabla_{\theta} V^{\pi}(s) \\
	=&\nabla_{\theta}(\sum_{a \in \mathcal{A}} \pi_{\theta}(a \mid s) Q^{\pi}(s,a)) \\
	=&\sum_{a \in \mathcal{A}}\left(\nabla_{\theta} \pi_{\theta}(a \mid s) Q^{\pi}(s, a)+\pi_{\theta}(a \mid s) \nabla_{\theta} Q^{\pi}(s, a)\right) \\
	=&\sum_{a \in \mathcal{A}}(\nabla_{\theta} \pi_{\theta}(a \mid s) Q^{\pi}(s, a)+\pi_{\theta}(a \mid s) \nabla_{\theta} \sum_{s^{\prime}, r} P\left(s^{\prime}, r \mid s, a\right)\left(r+V^{\pi}\left(s^{\prime}\right)\right)) \\
	=& \sum_{a \in \mathcal{A}}(\nabla_{\theta} \pi_{\theta}(a \mid s) Q^{\pi}(s, a)+\pi_{\theta}(a \mid s) \sum_{s^{\prime}} P\left(s^{\prime} \mid s, a\right) \nabla_{\theta} V^{\pi}\left(s^{\prime}\right)) \\
	=& \phi(s)+\sum_{s^{\prime}} \sum_{a} \pi_{\theta}(a \mid s) P\left(s^{\prime} \mid s, a\right) \nabla_{\theta} V^{\pi}\left(s^{\prime}\right) \\
	=& \phi(s)+\sum \rho^{\pi}\left(s \rightarrow s^{\prime}, 1\right) \nabla_{\theta} V^{\pi}\left(s^{\prime}\right) \\
	=& \phi(s)+\sum \rho^{\pi}\left(s \rightarrow s^{\prime}, 1\right)\left[\phi\left(s^{\prime}\right)+\sum \rho^{\pi}\left(s^{\prime} \rightarrow s^{\prime \prime}, 1\right) \nabla_{\theta} V^{\pi}\left(s^{\prime \prime}\right)\right] \\
	=& \phi(s)+\sum_{1} \rho^{\pi}\left(s \rightarrow s^{\prime}, 1\right) \phi\left(s^{\prime}\right)+\sum_{1 \prime} \rho^{\pi}\left(s \rightarrow s^{\prime \prime}, 2\right) \nabla_{\theta} V^{\pi}\left(s^{\prime \prime}\right) \\
	=& \text{- .; Repeatedly unrolling the part of} \nabla_{\theta} V^{\pi}(.) \\
	=& \sum_{x \in \mathcal{S}} \sum_{k=0}^{\infty} \rho^{\pi}(s \rightarrow x, k) \phi(x)
	\end{aligned}
	\end{equation}
	
	By plugging above into the objective function $J(\theta)$, we are getting the following:
	
	\begin{equation}
	\begin{aligned}
	\nabla_{\theta} J(\theta)=&\nabla_{\theta} V^{\pi}\left(s_{0}\right) \\
	=& \sum_{s} \sum_{k=0}^{\infty} \rho^{\pi}\left(s_{0} \rightarrow s, k\right) \phi(s) \\
	=& \sum_{s} \eta(s) \phi(s) \\
	=& \left(\sum_{s} \eta(s)\right) \sum_{s} \frac{\eta(s)}{\sum_{s} \eta(s)} \phi(s) \\
	\propto&  \sum_{s} \frac{\eta(s)}{\sum_{s} \eta(s)} \phi(s) \\
	=& \sum_{s} d^{\pi}(s) \sum_{a} \nabla_{\theta} \pi_{\theta}(a \mid s) Q^{\pi}(s, a) \\
	=& \sum_{s \in \mathcal{S}} d^{\pi}(s) \sum_{a \in \mathcal{A}} \pi_{\theta}(a \mid s) Q^{\pi}(s, a) \frac{\nabla_{\theta} \pi_{\theta}(a \mid s)}{\pi_{\theta}(a \mid s)} \\
	=& \mathbb{E}_{s \sim d^{\pi}, a \sim \pi_{\theta}}\left[Q^{\pi}(s, a) \nabla_{\theta} \log \pi_{\theta}(a \mid s)\right]
	\end{aligned}
	\end{equation}
	
	Where $d^{\pi}(s)=\frac{\eta(s)}{\sum_{s} \eta(s)}$ is stationary distribution. $\sum_{s} \eta(s)$ is a constant. The derivation process is carried out through the proof in the book \parencite{Sutton2018}, from which we can understand why the policy gradient theorem is correct. 
	
	\section{Advatage Function}
	$\hat{A}_t = - V(s_t) + r_t + \gamma r_{t+1} + \dots + \gamma^{T-t+1}r_{T-1} + \gamma^{T-t}V(s_T)$
  
	\chapter{Algorithms} \label{appendices-algorithms}
  \section{Single-Agent Reinforcement Learning}
	\subsection{DQN} \label{appendix:dqn}
	For completeness, the DQN algorithm used in single-agent bilateral negotiation
	
	\begin{algorithm}[H]
	\SetAlgoLined
	Initialize replay buffer $D$ to capacity $N$\;
	Initialize action-value function $Q$ with random weights $\theta$\;
	Initialize target action-value function $\hat{Q}$ with weight $\theta^{-}=\theta$\;
	\For{episode = 1, M}{
	Receive state from simulator $s_1 = \{x_1\}$ and preprocessed state $\phi_{1}=\phi\left(s_{1}\right)$\;
		\For{t = 1, T}{
			With probability $\omega$ select a random action $a_t$ (first step)\;
			otherwise select $a_{t}=\operatorname{argmax}_{a} Q\left(\phi\left(s_{t}\right), a ; \theta\right)$\;
			Execute action $a_t$ in siumulator and observe reward $r_t$ and new state $x_{t+1}$\;		
			Set $s_{t+1} = s_t, a_t, x_{t+1}$ and preprocess $\phi_{t+1} = \phi(s_{t+1})$ \;
			Store transitions $(\phi_j, a_j, r_j, \phi_{t+1})$ in $D$ \;
			Sample random minibatch of transitions $(\phi_j, a_j, r_j, \phi_{j+1})$ from $D$\;
			Set $y_{j}=\left\{\begin{array}{cc}r_{j} & \text { if episode terminates at step } \mathrm{j}+1 \\ r_{j}+\gamma \max _{a^{\prime}} \hat{Q}\left(\phi_{j+1}, a^{\prime} ; \theta^{-}\right) & \text {otherwise }\end{array}\right.$\;
			Perform a gradient descent step on $\left(y_{j}-Q\left(\phi_{j}, a_{j} ; \theta\right)\right)^{2}$ with respect to the network parameters\;
			Every $C$ steps reset $\hat{Q} = Q$\;
		}
	}
	\caption{Deep Q-learning with experience replay}
	\end{algorithm}
	\subsection{PPO} \label{appendix-algorithm-ppo}
	\begin{algorithm}[H]
	\For{iteration = 1 to $N$}{
		\For{actor = 1 to $N$}{
			Run policy $\pi_{\theta_{\text {old}}}$ in evironment for $T$ timesteps \;
			Compute advantage estimates $\hat{A}_1 \dots \hat{A}_T$ \;
		}
		Optimize surrogate $L$ wrt $\theta$, with $K$ epochs and minibatch size $M \leq NT$ \;
		$\theta_{\text{old}} \leftarrow \theta$ \;
	}
	
	\caption{PPO, Actor-Critic Style []}
	\end{algorithm}
	
	\subsection{DDPG}
	\begin{algorithm}[H]
  \SetAlgoLined
	Randomly initialize critic network $Q(s, a \mid \theta)$ and actor $\mu(s \mid \omega)$ \;
	Initialize target network $Q^{\prime}$ and $\mu^{\prime}$ with weights $\theta^{\prime} \leftarrow \theta$, $\omega^{\prime} \leftarrow \omega$ \;
	Initialize replay buffer $mathcal{D}$\;
	
	\For{episode = 1 to M}{
		Initialize $a$ random process $\mathcal{N}$ for action exploration \;
		Receive initial observation state $s_1$ \;
		\For{t = 1 to T}{
			Select action $a_t = \mu(s_t \mid \omega) + \mathcal{N}_t$ according to the current policy and exploration noise \;
			Execute action $a_t$ and observe reward $r_t$ and observe new state $s_{t+1}$ \;
			Store transition $(s_t, a_t, r_t, s_t{t+1})$ in $\mathcal{D}$ \;
			Sample a random minibatch of $N$ transitions $(s_i, a_i, r_i, s_{i+1})$ from $\mathcal{N}$ \;
			Set $y_i = r_i + \gamma Q^{\prime}(s_{i+1}, \mu^{\prime}(s_{i+1} \mid \omega^{\prime}) \mid \theta^{\prime})$ \;
			Update critic by minimizing the loss: $L=\frac{1}{N} \sum_{i}\left(y_{i}-Q\left(s_{i}, a_{i} \mid \theta\right)\right)^{2}$ \;
			Update the actor policy using the sampled policy gradient:
			\begin{equation}
			\left.\left.\nabla_{\omega} J \approx \frac{1}{N} \sum_{i} \nabla_{\theta} Q\left(s, a \mid \theta\right)\right|_{s=s_{i}, a=\mu\left(s_{i}\right)} \nabla_{\omega} \mu\left(s \mid \omega\right)\right|_{s_{i}}
			\end{equation}
			Update the target networks:
			\begin{equation}
			\begin{aligned}
			\theta^{\prime} & \leftarrow \tau \theta+(1-\tau) \theta^{\prime} \\
			\omega^{\prime} & \leftarrow \tau \omega+(1-\tau) \omega^{\prime}
			\end{aligned}
			\end{equation}
		}
	}
	\caption{Deep Deterministic Policy Gradient(DDPG) algorithm\parencite{Weng2018}}
	\end{algorithm}
	
  \section{Multi-Agent Reinforcement Learning}
	\subsection{\gls{maddpg}} \label{appendix:algorithms:maddpg}
	For completeness, the MADDPG algorithm used in \gls{scml} is provided below.
	
	\begin{algorithm}[H]
  \SetAlgoLined
  \KwData{State comes from simulator \gls{scml}}
  \KwResult{action sequence, proposal offer or set dynamical range of negotiation issues}
	
	\For{episode = 1 to M}{
		Initialize a random process $\mathcal{N}$ for action exploration\;
		Receive the intial state from the Simulator\;
		\For{t = 1 to max-episode-length}{
				for each agent $i$, select action $a_{i}=\boldsymbol{\mu}_{\theta_{i}}\left(o_{i}\right)+\mathcal{N}_{t}$ w.r.t. the current policy and exploration.\;
				Execute joint actions $a=\left(a_{1}, \ldots, a_{N}\right)$ and get the reward $r$ and new state $\mathbf{s}^{\prime}$\;
				Store $\left(\mathbf{s}, a, r, \mathbf{s}^{\prime}\right)$ in replay buffer $\mathcal{D}$\;
				$\mathbf{s} \leftarrow \mathbf{s}^{\prime}$\;
				\For{agent $i=1$ to $N$}{
					Sample a random minibatch of samples $\mathcal{B}$ $\left(\mathbf{s}^{j}, a^{j}, r^{j}, \mathbf{s}^{\prime j}\right)$ from $\mathcal{D}$\;
					Set $y^{j}=r_{i}^{j}+\left.\gamma Q_{i}^{\mu^{\prime}}\left(\mathbf{s}^{\prime j}, a_{1}^{\prime}, \ldots, a_{N}^{\prime}\right)\right|_{a_{k}^{\prime}=\boldsymbol{\mu}_{k}^{\prime}\left(o_{k}^{j}\right)}$ \;
					Update critic by minimizing the loss $\mathcal{L}\left(\theta_{i}\right)=\frac{1}{B} \sum_{j}\left(y^{j}-Q_{i}^{\boldsymbol{\mu}}\left(\mathbf{s}^{j}, a_{1}^{j}, \ldots, a_{N}^{j}\right)\right)^{2}$ \;
					Update actor using the sampled policy gradient:
					\begin{equation}
						\left.\nabla_{\theta_{i}} J \approx \frac{1}{B} \sum_{j} \nabla_{\theta_{i}} \boldsymbol{\mu}_{i}\left(o_{i}^{j}\right) \nabla_{a_{i}} Q_{i}^{\mu}\left(\mathbf{s}^{j}, a_{1}^{j}, \ldots, a_{i}, \ldots, a_{N}^{j}\right)\right|_{a_{i}=\boldsymbol{\mu}_{i}\left(o_{i}^{j}\right)}
					\end{equation}
				}
				Update target network parameters for each agent i: $\theta_{i}^{\prime} \leftarrow \tau \theta_{i}+(1-\tau) \theta_{i}^{\prime}$
			}
	}
  \caption{Multi-Agent Deep Deterministic Policy Gradient for N agents}
\end{algorithm}

	\subsection{\gls{qmix}}	
	\begin{algorithm}[H]
	Initialise $\theta$, the parameters of mixing network, agent networks and hypernetwork\;
	Set the learning rate $\alpha$ and replay buffer $\mathcal{D}$\;
	step = 0, $\theta^{-}=\theta$\;
	\For{step = 0 to $step_{max}$}{
		$s_{0}=\text{initial state}$
		\While{$s_{t} \neq \text { terminal }$ and $t<\text { episode limit }$}{
			\For{each agent $a$}{
			$\tau_{t}^{a}=\tau_{t-1}^{a} \cup\left\{\left(o_{t}, u_{t-1}\right)\right\}$ \;
			$\epsilon=\text { epsilon-schedule }(\text { step })$ \;
			$u_{t}^{a}=\left\{\begin{array}{ll}\operatorname{argmax}_{u_{t}^{a}} Q\left(\tau_{t}^{a}, u_{t}^{a}\right) & \text { with probability } 1-\epsilon \\ \operatorname{randint}(1,|U|) & \text { with probability } \epsilon\end{array}\right.$ \;
			}
			Get reward $r_t$ and next state $s_{t+1}$ \;
			$\mathcal{D}=\mathcal{D} \cup\left\{\left(s_{t}, \mathbf{u}_{t}, r_{t}, s_{t+1}\right)\right\}$ \;
			$t=t+1, step=step+1$\;
		}
		\If{$|\mathcal{D}|$>\text { batch-size }}{
			$\mathrm{b} \leftarrow \text { random batch of episodes from } \mathcal{D}$ \;
			\For{each timestep $t$ in each episode in batch $b$}{
				$Q_{t o t}=\text { Mixing-network }\left(Q_{1}\left(\tau_{t}^{1}, u_{t}^{1}\right), \ldots, Q_{n}\left(\tau_{t}^{n}, u_{t}^{n}\right) ; \text { Hypernetwork }\left(s_{t} ; \theta\right)\right)$ \;
				Calculate target $Q_{tot}$ using Mixing-network with Hypernetwork$\left.\left(s_{t} ; \theta^{-}\right)\right)$ \;
		}
			$\Delta Q_{tot} = y^{tot} - Q_{tot} $ \;
			$\Delta \theta = \nabla_{\theta}(\Delta Q_{tot})^{2}$ \;
			$\theta = \theta - \alpha \Delta \theta$ \;
		}
		\If{update-interval steps have passed}{
			$\theta^{-} = \theta$\;
		}
	}
	
	\caption{QMIX Algorithm\parencite{DBLP:journals/corr/abs-2003-08839}}
	\end{algorithm}
	
\end{appendices}