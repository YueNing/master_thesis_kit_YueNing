\chapter{Methods and Experiments} \label{methods-and-experiments}

\section{Single-Agent Bilateral Negotiation Environment (\gls{sbe})}
In this environment, agent represents the negotiator in negotiation mechanism.

\subsection{Independent Negotiator in NegMAS}
In the environment has just single learnable \gls{drl} negotiator. All \gls{rl} algorithms with discrete action space can be tested in this specific environment. In the experiment of this thesis, \gls{dqn} and \gls{ppo} were tested in four learning cases:
\begin{itemize}
	\item single issue, acceptance strategy
	\item single issue, offer strategy
	\item multi-issues, acceptance strategy
	\item multi-issues, offer startegy
\end{itemize}

The training logic of DQN is shown in Figure \ref{fig:dqn}, the detailed description of algorithm is shown in appendix \ref{appendix:dqn}.
\begin{figure}[htbp]
\centering
\includegraphics[width=0.30\textwidth]{./images/dqn.png}
\caption{Training logic of DQN}
\label{fig:dqn}
\end{figure}

\subsection{Experiment} \label{sbe:experiment}
Figure \ref{fig:bilateral-negotiation} diagrams the Game in \gls{sbe}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.50\textwidth]{./images/bilateral-negotiation.png}
\caption{Bilateral Negotiation Game in \gls{sbe}, My Deep Reinforcement Learning Negotiator vs. Aspiration Negotiator}
\label{fig:bilateral-negotiation}
\end{figure}

Negotiation mechanism is \gls{saom}, split the learning strategy as two parts, acceptance strategy and offer strategy, 

\paragraph{Acceptance strategy} actions of agent are \texttt{Accept offer}, \texttt{Wait} and \texttt{Reject offer}. The variables observed by the agent are current offer of opponent and current time(running time, or relative round of negotiation mechanism).

\paragraph{Offer strategy} Actions of negotiator are set of outcome in negotiation mechanism. The observation is same as observation defined in the acceptance strategy. Before feeding variables into the algorithm, action and observation are normalized.

\subsubsection{single issue}
The training environment is based on the \gls{sbe} and sets concrete limits and attributes for it, which are defined in Tabel \ref{}.

Algorithms \gls{dqn} (blue) and \gls{ppo} (red) are tested in the cases of single issue . Mean episode reward of case  \textbf{single issue, acceptance strategy} is shown in \ref{fig:single-issue}

\begin{figure*}
   \centering
\begin{tabular}{mean reward of single issue negotiation}
\includegraphics[width=.45\textwidth]{./images/ac_s.png}&
\includegraphics[width=.45\textwidth]{./images/ac_s_wall.png}\\
\includegraphics[width=.45\textwidth]{./images/of_s.png}&
\includegraphics[width=.45\textwidth]{./images/of_s_wall.png}\\
\end{tabular}
    \caption{Mean Reward of Acceptance Strategy and Offer Strategy}
    \label{fig:single-issue} % I can do without the label too
\end{figure*}

\subsubsection{multi issues}

\subsection{Evaluation}

\section{Multi-Agent Concurrent Bilateral Negotiation Environment (\gls{mcbe})}
In this environment, agent represents the factory manager and negotiation controller in standard \gls{scml} and \gls{scml} OneShot, respectively.

The agent interacting with environment may be have many related trainable agents as the part of learner(e.g. one seller, one buyer) in the model. The detail of interactive logic is shown below in \ref{fig:interacting-logic-scml}

\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\textwidth]{./images/scnk.png}
\caption{Interactive logic based on the perspective of \gls{scml}. N: The maximum number of concurrent negotiations for a single agent}
\label{fig:interacting-logic-scml}
\end{figure}

\subsection{\gls{maddpg} in \gls{scml}} \label{methods:maddpg}
In the standard scml environment, two questions are tried to be fixed with maddpg. 

\paragraph{Question 1: Dynamical Range Of Negotiation Issues}At the beginning of every negotiation in simulator, agent will determine the range which constraints value interval for negotiation issues. In the experiment, the negotiation issues are \textbf{QUANTITY}, \textbf{PRICE} and \textbf{TIME}. After creating the simulation world, simulator determines the minimum and maximum values for each negotiation issue taken by the entire simulation episode, such as value of \textbf{QUANTITY} between (1, 10), \textbf{PRICE} between (0, 100) and \textbf{TIME} between (0, 100). However, for every negotiation mechanism created beside the entire simulation episode, it has dynamical range of negotiation issues which affect the negotiation process. This question was raised based on such a situation.

\paragraph{Question 2: The Offer For Every Step}From the description of question 1, we can find, action obtained by algorithm influence only finite the state of environment. Agent(Factory Manager) can not control the function \textbf{proposal} of every negotiation step. Every negotiation step has always been controlled by heuristic negotiation strategy. Intuitively, the main influence comes from the joint action of each step of the negotiation. Hence, question 2 \textbf{The Offer For Every Step} is proposed naturally. After the basic problem is determined, how to design becomes the current problem. 

From an algorithm perspective, the data flow of the model is shown in \ref{fig:method-maddpg-scml}. 
\gls{maddpg} used in \gls{scml}, one trainable agent defined in \gls{maddpg} is not equal to the agent defined in \gls{scml}.
It create $D$ process for action exploration, in this environment, dynamical range issues are needed to be explored 
The basic concepts of \gls{maddpg} are introduced in chapter background \ref{background:maddpg}. 

\begin{figure}[htbp]
\centering
\includegraphics[width=1.00\textwidth]{./images/scml-maddpg.png}
\caption{\gls{maddpg} used in \gls{mcbe}}
\label{fig:method-maddpg-scml}
\end{figure}

Actors output actions as inputs to related agents interacting with the environment. Agents interacting with environment outputs the observation and reward as the inputs to related agents trained in the model. Details of the algorithm are described in the appendix \ref{appendix:algorithms:maddpg}.  


\subsection{\gls{qmix} in \gls{scml-oneshot}} \label{methods:qmix-scml-oneshot}

The world created by \gls{scml-oneshot} is described in detail in chapter background \ref{background-scml}. 

\paragraph{Question: The Offer For Every Step} Unlike in standard scml \textbf{Dynamical Range of Negotiation Issues} is controlled by agents, the system takes over the related control and access authority in scml oneshot. Hence, question 1 in the standard library does not need to be discussed here. Although the design of oneshot world is very different with the standard libray, the key question is also how to find the optimal sequence action(offer for every negotiation step).

In the current version \gls{qmix}, which is used in the experiment, one trainable agent is related to one negotiation session. When the agents are located in different locations in the scml world, the agents have different concurrent negotiation maximums. Since the agent \textbf{A1} shown in Figure \ref{fig:interacting-logic-scml} has three consumers, the maximum value of concurrent negotiations of the agent \textbf{A1} is 3. Based on this value, we need to create three trainable agents in algorithm, and each trainable agent control one negotiation session of interative agent.

Data flow is shown in \ref{fig:method-qmix-scml}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.80\textwidth]{./images/scml-qmix.png}
\caption{\gls{qmix} used in \gls{mcbe}}
\label{fig:method-qmix-scml}
\end{figure}

\subsection{Experiment}
\subsubsection{Concurrent Neogtiations in standard \gls{scml}}
Standard \gls{scml} is a complex simulation world, which contains various parts with specific functions. The breif description of this simulation is introduced in chapter Background \ref{background-scml}. The experiment of this thesis focus on only the Negotiation Manager of Decision-Maker Agent. The above mentioned method maddpg \ref{methods:maddpg} is used in this experiment. 
Scenario is diagramed in Figure \ref{fig:scenario-standard-scml}.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.80\textwidth]{./images/scenario-standard-scml.png}
\caption{M* represent My Component Based Agent with learner \gls{maddpg}, C* represent Opponent Agents, such as \gls{ind-dec-agent}}
\label{fig:scenario-standard-scml}
\end{figure}

\paragraph{Evaluation} Before evaluating the result of Question 2 \textbf{The Offer For Every Step} in section \ref{methods:maddpg}. The result of Question 1 \textbf{Dynamical Range Of Negotiation Issues} in section \ref{methods:maddpg} is shown in \ref{fig:dynamical-range-issues-maddpg}.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.6\textwidth]{./images/dynamic_range_issues_maddpg.png}
\caption{Scores of agents running in simulation world after training}
\label{fig:dynamical-range-issues-maddpg}
\end{figure}


\subsubsection{Concurrent Negotiations in OneShot \gls{scml}}
\gls{scml-oneshot} world is a new simpler world from standard scml. This world only cares about concurrent negotiation in supply chain management, and the agents used in this world can be easily transferred to standard scml. 
 
The breif description of this simulation world is introduced in chapter Background \ref{background-scml}. This part of the experiment only focuses on negotiation. The above mentioned method qmix \ref{methods:qmix-scml-oneshot} is used in this experiment. 

\paragraph{self-play}

Scenario is diagramed in Figure \ref{fig:scenario-oneshot-scml-self-play}.
\begin{figure}[htbp]
\centering
\includegraphics[width=0.80\textwidth]{./images/scenario-oneshot-scml-self-play.png}
\caption{M* represent My Component Based Agent with learner \gls{qmix}}
\label{fig:scenario-oneshot-scml-self-play}
\end{figure}

Episode mean reward curve is shown in \ref{fig:oneshot-self-play}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.55\textwidth]{./images/oneshot_self_play.png}
\caption{Episode mean reward of self paly under SCML OneShot}
\label{fig:oneshot-self-play}
\end{figure}

\paragraph{play with other agent}

Scenario is diagramed in Figure \ref{fig:scenario-oneshot-scml-play-with-greedy}.
\begin{figure}[htbp]
\centering
\includegraphics[width=0.80\textwidth]{./images/scenario-oneshot-scml-play-with-greedy.png}
\caption{M* represent My Component Based Agent with learner \gls{qmix}, G* represent GreedyOneShotAgent}
\label{fig:scenario-oneshot-scml-play-with-greedy}
\end{figure}


Episode mean reward curve is shown in \ref{fig:oneshot-my-vs-greedy}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.65\textwidth]{./images/oneshot_my_vs_greedy.png}
\caption{Episode mean reward of my agent vs GreedyOneShotAgent under SCML OneShot}
\label{fig:oneshot-my-vs-greedy}
\end{figure}

\paragraph{Evaluation}


\section{Conclusion}