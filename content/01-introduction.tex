\chapter{Introduction}
Computer software and hardware development leads to the appearance of non-human software agents. An agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through effectors\parencite{RePEc:aes:infoec:v:xi:y:2007:i:4:p:115-118}.

In economics, autonomous agent can be considered as a specific type of agent, with a focus on generating economic value. This technology will be at the forefront of the next industrial revolution, affecting numerous billion dollar industries such as transportation and mobility, finance, supply chain, energy trading, social networks and marketplaces and e-commerce. The detail about application field of autonomous agent will be listed in chapter Background \ref{Chapter:Background}. All the work in this article is centered on the application of automatic agent in autonomous negotiation and in the supply chain.

A supply chain is a network of suppliers, factories, warehouses, distribution centers and retailers, through which raw materials are acquired, transformed, produced and delivered to the Customer. In the network we could find many entities whose can be considered as agents mentioned in the Multi-agent systems(\gls{mas}). \gls{mas} is suitable the domains that involve interactions between different people or organizations with different (possibly conflicting) goals and proprietary information. Supply chain network is a typical \gls{mas}. There are many approaches are proposed in order to solve the problem in this system, such as negotiation-based Multi-agent System\parencite{Chen1999ANM, DBLP:journals/corr/abs-1210-3375}. Supply chain management(\gls{scm}) involves the following stages: planning, procurement, production, delivery and return. At each stage it has its own processes, problems and solutions\parencite{Darja2011}.

\gls{scm} world designed in simulator called supply chain management leagure(\gls{scml}) based on opensource package \gls{negmas} by \texttt{Yasser Mohammad} simulates a supply chain consisting of multiple factories that buy and sell products from one another. The factories are represented by autonomous agents that act as factory managers. Agents are given some target quantity to either buy or sell and they negotiate with other agents to secure the needed supplies or sales. Their goal is to turn a profit, and the agent with the highest profit (averaged over multiple simulations) wins. \gls{scml} is characterized by profit-maximizing agents that inhabit a complex, dynamic, negotiation environment\parencite{Mohammad2019}. All definitions related to \gls{scml} and \gls{negmas} will be provided in Background \ref{background-scml} and \ref{background:negmas}. Because all subsequent work in this thesis will be carried out around these platforms. Before discussing the details of the work of this thesis, we need to give specific motivations for the work. 

\subsection{Motivation}
Negotiation is a complex problem, in which the variety of settings and opponents that may be encountered prohibits the use of a single predefined negotiation strategy. Hence, the agent should be able to learn such a strategy autonomously\parencite{Bakker2019RLBOAAM}. By autonomy it means “independent or self-governing”. In the context of an agent, this means it can act without constant interference from its owner\parencite{Fetch.ai2019}. Autonomous negotiation agent is meaningful in many realistic environment, such as \gls{scm}. The current increased hardware resources make it possible to use computer systems to model real-world environments to evaluate the problems. According to the modeled environment, it will be easier to find more possible solutions with the help of machine learning technology.

In the work of this thesis, some modeled negotiation environments have been developed, such as single-agent environment(bilateral negotiation), to analyze whether deep reinforcement learning can be used to allow agents to automatically learn some good strategies. Compared with the single agent environment, in a supply chain environment, there are many agents with the same goals. After analyzing the simple environment, the situation is needed to be explored whether multi-agent deep reinforcement learning methods can be used to obtain better results in multi-agent environment(concurrent bilateral negotiations).

\textbf{What is the significance of applying Deep Reinforcement Learning in supply chain management?}

Due to the great success of Alphago Zero\parencite{Silver2017} and OpenAI Five\parencite{OpenAI2019}, reinforcement learning has entered a new historical stage. As a general machine learning method, whether it can be used to improve the management ability of factories in the supply chain world is a very natural idea. However, deep reinforcement learning has many problems. Whether it is effective or not in supply chain needs to be tested through experiments. The supply chain world is a very typical multi-agent system, and the goal of maximizing profits is also a typical multi-step decision-making problem. Hence, many multi-agent reinforcement learning methods have great practical significance and research value here.

\textbf{How good strategy can be learned by Deep Reinforcement Learning(\gls{drl}) in single agent environment (bilateral negotiation)?}

Before testing deep reinforcement learning in a multi-agent environment, single-agent bilateral negotiation is a better useful test environment. The result will help to analyze the role of algorithm in autonomous negotiation game. It has a certain practical significance. In recent years, many single agent independent \gls{drl} algorithms has been proposed, such as DQN, PG, A2C etc. The application of these algorithms in automatic negotiation will be a very meaningful study. The performance of these algorithms will provide inspiration for the analysis of more complex negotiation environment.

\textbf{How good strategy can be learned by multi-agent deep reinforcement learning in multi-agent environment (concurrent negotiation)?}

This question is the key question of this thesis. Due to the successful application of \gls{drl} in complex game environments, exploring its significance in complex negotiation environments will accelerate the research progress in the field of automatic negotiation. Beside the value of research, definitively, it also has the economic value. By comparing with other benchmark negotiators or agents, the importance of \gls{drl} can be evaluated.

\textbf{What is the difference between deep reinforcement learning\gls{drl} strategies and other heuristic strategies?}

Researchers has proposed many heuristic strategies, such as time-based and behavior-based negotiation strategies, which will be introduced in chapter related work \ref{related-work:heuristic-negotiation}. Comparing these strategies can help achieve better strategies, and agents based on these strategies can be used as opponents of \gls{rl} agents. The results of the evaluation will help to understand the reasons for using deep reinforcement learning.

In order to obtain and analyze the results of the above four questions, it is necessary to understand the simulation logic of the simulator \gls{negmas} and \gls{scml} as a prerequisite for the experiment.

\subsection{Outline of this Work}
In the following, the other chapters of the work of thesis are listed and their content briefly presented.

\paragraph{Chapter 2: Background:}
This chapter contains basic knowledge and concepts that are necessary for understanding the work of this thesis. First, some concepts from game theory are introduced. These mentioned concepts are often discussed and used in autonomous negotiation. Second, utility function and several important negotiation mechanisms are described in the section autonomous negotiation. In addition, the basics and the historical development of artificial intelligence are presented in two individual sections. The focus of these sections are basic knowledge of reinforcement learning. Then, \gls{negmas} and \gls{scml} are introduced relatively clearly, which are two important packages(simulator) that simulate autonomous negotiation and supply chain world, respectively. Finally, this chapter contains the brief introduction of some tools(\gls{openai gym}) and training tools(Ray) used to create a training environment.

\paragraph{Chapter 3: Related Works}
In this chapter, some published matter which technically relates to the proposed work in this thesis will be discussed. These publication will be divided as three categories: Heuristic Negotiation Strategies for Autonomous Negotiation, Reinforcement Learning used in Autonomous Negotiations and Challenges in Deep Reinforcement Learning. Heuristic negotiation strategies, such as time-based and behavior-based, will be discussed first. In the second part, some deep reinforcement learning frameworks for autonomous negotiation will be introduced. Finally, it is about typical challenges(e.g., design of reward function) in the field of reinforcement learning. Some solutions will be given in this section.

\paragraph{Chapter 4: Analysis}
The task of the thesis is studied in detail in the chapter "Analysis". The main content of this chapter is to explain how to design two custom training environments: single-agent(bilateral negotiation) and multi-agent(concurrent negotiation) environment. In addition to the model of the training environment, this chapter will also introduce the design of the observation space, action space and reward function in the algorithm.
This means that the characteristics of the algorithm need to be analyzed in advance.

\paragraph{Chapter 5: Methods and Experiments}
The detailed configuration of the experimental environment is explained in this chapter. The specific hyperparameters and training process of the algorithm will also be explained. Finally, the experimental results are presented and evaluated, and compared with other algorithms. 

\paragraph{Chapter 6: Conclusions and Future Work}
In the last chapter, the work of this thesis will be summarized and the areas for improvement will be pointed out. Finally, that chapter will provide some directions for future work.