\chapter{Conclusions and Future Work}
In this thesis, it mainly comprises three works. First, two custom training environment(i.g., \gls{sbe} and \gls{mcbe}) are developed for  training agent in the environment of bilateral negotiation and concurrent bilateral negotiations, respectively. Second, it testes baseline \gls{rl} algorithms(scuh as, DQN, PPO) in \gls{sbe} and testes two multi-agent drl algorithms(i.e., maddpg and qmix) in \gls{mcbe}. Finally, the results(e.g., mean episode reward) are presented and evaluated. There are many parts that can be analyzed and improved in the future.

\section{Other goals}
In the SCM league, profit-maximizing is the goal of RL-agent, in game theory we could find many other goals, such as welfare-maximization, pareto optimality. How to achieve these goals with RL-methods based on the developed environments \gls{sbe} and \gls{mcbe}? The key part is the design of the reward function. For pareto optimal, one idea is that the distance between the utility of the current offer and the utility of the pareto optimal offer can be expressed as the reward. The details of the reward function design for other goals can be regarded as the main question in the future.

\section{Evaluation}
The evaluation work of this thesis focuses on two metrics: mean episode reward and score. It is very inadequate for evaluating multiple agents. There are many metrics for the evaluation of multi-agent system, not only for the \gls{rl} multi-agent, but also for the environment. These metrics and methods can speed up the development of available \gls{rl} multi-agent. 

Many characteristics, such as \textbf{Complexity of environment}, \textbf{Rationality of agent}, \textbf{Autonomy}, \textbf{Reactivity} and \textbf{Adaptability}, first were proposed in the paper\parencite{Bitonto2010}.

\textbf{Complexity of environment:} Many parameters were proposed in the paper to describe the complexity:
\begin{itemize}
\item \textbf{Inaccessibility} It expresses the difficulty in gaining complete access to the resources in its environment.
\item \textbf{Instability} It expresses the way the environment evolves. In other words, the difficulty in perceiving changes in the environment. The faster and more unpredictably the environment changes, the more complex it is.
\end{itemize}

The future direction of work may be to find suitable methods to qualify and quantify these metrics.

\section{Design of reward function}
Reward function is an important part of realizing RL-Agent. The future work can focus on the design of a more effective reward function. Imitation reinforcement learning is a powerful method through which an appropriate reward function can be deduced based on expert demonstration. This deduced reward function can be set as a conventional reward function used in traditional RL algorithms. In the future, the effectiveness and implementation process of this method will be explored.

\section{Complex environment}
Currently, it is only possible to train multi-agents in the very simple \gls{scm} world. How to effectively train multi-agents in a complex environment is very important question. We can do some exploratory work in this area in the future.

\section{Huge scale high performance learning}
In addition to improving algorithms, there are many engineering methods that help train the agents in large-scale environments and speed up the learning process of the agents. DeepMind developed a tool called \textbf{Reverb}\parencite{cassirer2021reverb}, which is an efficient and easy-to-use data storage and transport system designed for machine learning research. Reverb is primarily used as an experience replay system for distributed reinforcement learning algorithms. In the experiment of this thesis, due to the storage of a lot of experience replay, very large memory usage is a problem. Reverb can be used as an engineering tool to solve this problem. There are some experiments that can be carried out in the future.
