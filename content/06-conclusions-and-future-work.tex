\chapter{Conclusions and Future Work}

\section{Others goal}
In the SCM league, profit-maximizing is the goal of RL-agent, in game theory we could get many goals, such as welfare-maximization, pareto optimality. How to achieve these goals with RL-methods based on the developed environments \gls{sbe} and \gls{mcbe}? The key part is the design of reward function. For pareto optimality, the reward can be set as the distance between the current utility of offer and the utility of pareto optimal offer. The details of the design of the reward function for other goals can be seen as a major issue in the future.

\section{Evaluation}
The evaluation work of this thesis focuses on two parameters: mean episode reward and score running in the \gls{scml} world. It is very inadequate for evaluating multiple agents. There are many metrics for the evaluation of multi-agent system, not only for the \gls{rl} multi-agent, also for the environment. These metrics and methods can speed up the development of available \gls{rl} multi-agent. 

Many characteristics, such as \textbf{Complexity of environment}, \textbf{Rationality of agent}, \textbf{Autonomy}, \textbf{Reactivity} and \textbf{Adaptability}, first proposed in the paper\parencite{Bitonto2010}.

\textbf{Complexity of environment:} Many parameters proposed in the paper  to describe the complexity:
\begin{itemize}
\item \textbf{Inaccessibility} It expresses the difficulty in gaining complete access to the resources in its environment.
\item \textbf{Instability} It expresses the way the environment evolves. In other words, the difficulty in perceiving changes in the environment. The faster and more unpredictably the environment changes, the more complex it is.
\end{itemize}

The future direction of work may be to find suitable methods to qualify and quantify these metrics.

\section{Design of reward function}
Reward function is an important part of realizing of RL-Agent. The future work can focus on the design of a more effective reward function. Imitation reinforcement learning is a meaningful method through which an appropriate reward function can be deduced based on expert knowledge. This type of reward function can be set as a conventional reward function used in traditional RL algorithms. In the future, the effectiveness and implementation process of this method will be explored.

\section{Complex environment}
Currently, it is only possible to train multi-agents in the very simple \gls{scm} world. How to effectively train multi-agents in a complex environment is very important question. 

\section{Huge scale high performance learning}
In addition to improving algorithms, there are many engineering methods that help train the agents in large-scale environments and speed up the learning process of the agents. Reverb is 
Ray and reverb
