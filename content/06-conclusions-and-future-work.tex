\chapter{Conclusions and Future Work}

\section{Others goal}
In the SCM league, profit-maximizing is the goal of RL-agent, in game theory we could get many goals, such as welfare-maximization, pareto optimality. How to achieve these goals with RL-methods based on the developed environments \gls{sbe} and \gls{mcbe}? The key part is the design of reward function. For pareto optimality, the reward can be set as the distance between the current utility of offer and the utility of pareto optimal offer. The details of the design of the reward function can be seen as a major issue in the future.

\section{Evaluation}
The evaluation work of this thesis focuses on two parameters: mean episode reward and score running in the \gls{scml} world. It is very inadequate for evaluating multiple agents. There are many research works about the evaluation of multi-agent, and it is not just about the \gls{rl} multi-agent.
The metrics and evaluation methods proposed in the field of multi-agent can be used to evaluate the \gls{rl} multi-agent. These metrics and methods can speed up the development of available \gls{rl} multi-agent. 

Many metrics in the filed multi agent could be used for evaluating the agents proposed in this paper, such as....

\section{Design of reward function}
Reward function is an important part of realizing of RL-Agent, In the future can develop a more effective reward function, such as method proposed in the paper by [].

\section{Complex environment}
Currently, it is only possible to train multi-agents in the very simple \gls{scm} world. How to effectively train multi-agents in a complex environment is very important question. 

\section{Huge scale high performance learning}
In addition to improving algorithms, there are many engineering methods that help train the agents in large-scale environments and speed up the learning process of the agents. Reverb is 
Ray and reverb
