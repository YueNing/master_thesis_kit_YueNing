\chapter{Related Works}
In this chapter, related work on the relevant topics of this work is presented and discussed. The topics include autonomous negotiation, multi-agent reinforcement learning. At the last section, the work of reinforcement learning used in autonomous negotiation is presented. 

\section{Heuristic Negotiation Strategies for Autonomous Negotiation}
\subsection{Time-based Strategy (Aspiration Negotiator)}
Aspirations are the specific goals in a negotiation that a negoitator wishes to achieve as part of an agreement. Empricial evidence has shown that negotiators with higher aspirations tend to achieve better bargaining results. First, aspiration of negotiator can help to determine the outer limit of what negotiator will request. Second, optimistic aspirations can make the negotiators to work harder\parencite{Schneider2004}. Many aspiration negotiators are time dependent, such as Boulware(with concession factor $e=1/2$), Hardliner($e=0$), Conceder Linear($e=1$) and Conceder($e=2$)\parencite{FARATIN1998159}. Boulwarism is the tactic of making a "take-it-or-leave-it" offer in a negotiation, with no further concessions or discussion. It was named after General Electric's former vice president Lemuel Boulware, who promoted the strategy\parencite{William1991}.

The value of issue $j$ as the offer at time $t$ sent by agent a to agent b is modeled as follows:
\begin{equation}
x_{a \rightarrow b}^{t}[j]=\left\{\begin{array}{ll}
\min _{j}^{a}+\alpha_{j}^{a}(t)\left(\max _{j}^{a}-\min _{j}^{a}\right) & \text { if } V_{j}^{a} \text { is decreasing } \\
\min _{j}^{a}+\left(1-\alpha_{j}^{a}(t)\right)\left(\max _{j}^{a}-\min _{j}^{a}\right) & \text { if } V_{j}^{a} \text { is increasing. }
\end{array}\right.
\end{equation}
Where $V_{j}$ denotes utility value of issue $j$ set as the offer, $\min _{j}$ and $\max _{j}$ mean the minimum and maximum value of offer $j$, respectively.

Figure \ref{fig:boulware-conceder} diagrams the convexity degree of $\alpha_j(t)$ used in the calcualtion model of offer's value.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.6\textwidth]{./images/boulware-conceder.png}
\caption{Computation of $\alpha(t)$. Time is presented as relative to $t_{max}$. Polynomial Function: $\alpha_{j}(t)=\kappa_{j}+\left(1-\kappa_{j}\right)\left(\min \left(t, t_{\max }\right) / t_{\max }\right)^{1 / e}$. $\kappa_{j}$ is minimum conceder of negotiator.}
\label{fig:boulware-conceder}
\end{figure}

The offer will always be between the value range($[min_j, max_j]$), at the beginning it will give the initial constant and when the time deadline is reached the tactic will suggest to offer the reservation value.


\subsection{Tit for tat strategies}

\subsection{Concurrent Negotiation Strategy (\gls{cns})}
In a concurrent negotiation environment, an agent will negotiate with many opponents at the same time(one-to-many). One issue is how to coordinate all these negotiations. The author of the paper \parencite{Williams12Concurrent} designed an intuitive model with two key parts, namely the \texttt{Coordinator} and
\texttt{Negotiation Thread}. 

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{./images/heuristic_concurrent_negotiation.png}
\caption{Architecture of the concurrent negotiation agent, best time: $t_i$ and utility value: $u_i$, probability distributions: $P$ \parencite{Williams12Concurrent}.}
\label{fig:heuristic-concurrent-negotiation}
\end{figure}

\textbf{Negotiation Threads:}The strategy of each negotiation thread is an extension of a recently published, principled, adaptive bilateral negotiation agent. This agent was designed to be used in a similarly complex environment, but only for negotiations against a single opponent.

\textbf{Coordinator:}The role of the coordinator is to calculate the best time, $t_i$ and utility value, $u_i$ at that time, for each thread. To do so, it uses the probability distributions received from the individual threads, which predict future utilities offered by the opponents.

%\subsection{Optimal Negotiation Strategies}
\subsection{Conclusion}
From the analysis of the heuristic negotiation strategy in a specific field, we can get some important parameters, such as time, offer by opponent, that need to be considered as the information used in the \gls{rl} algorithm.

\section{Reinforcement Learning used in Autonomous Negotiation}
\textbf{\gls{negosi}:} A novel algorithm named negotiation-based MARL
with sparse interactions (NegoSI) is presented by \texttt{Luowei Zhou}. In contrast to traditional sparse-interaction based MARL algorithms, NegoSI adopts the equilibrium concept and makes it possible for agents to select the non-strict Equilibrium Dominating Strategy Profile (non-strict EDSP) or Meta equilibrium for their joint actions \parencite{L2017NegoSI}.


\textbf{\gls{rlboa}:} From the paper \parencite{Bakker2019RLBOAAM} A Modular Reinforcement Learning Framework for Autonomous Negotiating Agents.

\textbf{\gls{anegma}:} Work by \parencite{bagga2020deep}. A novel DRL-inspired agent model called ANEGMA, which allows the buyer to develop an adaptive strategy to effectively use against its opponents (which use fixed-but-unknown strategies) during concurrent negotiations in an environment with incomplete information . 


\section{Challenges in Deep Reinforcement Learning}

\subsection{Sparse Reward}
Utility value as part of reward function
reward shaping
Curiosity Driven
Imitation Learning

\subsection{Non-stationary environment}
The strategy of single agent is changed during training
Multi-agent environment is non-statinal
Multi-agent deep reinforcement learning 

\subsection{Huge action space}
Action embedding, discrete action replaced by continious action space. 