\addchap*{Summary of Notation}
\markboth{Notation}{}
Lower case letters are used for the values of random variables and for scalar functions. Capital ltters are used for random variables and major algorithms variables.
\addsec*{General identifier}

\begin{tabular}{p{1.5cm} p{12cm}}
$s$ & state \\
$a$ & action(RL) or agent(autonomous negotiation)\\
$t$ & discrete time step \\
$o$ & observation \\
$\mT$ & final time step of an episode \\
$\pi$ & policy, decision-making rule \\
$\mu$ & policy, determinstic policy \\
$\gamma$ & discount-rate parameter or multiplication factor in utility function of an agent\\
$\alpha$ & learning rate or multiplication factor in utility function of an agent\\
$\varepsilon$ & probability of random action in $\varepsilon$-greedy policy \\
$\theta$ & parameters of policy(i.g., network) \\
$\omega$ & parameters of network \\
$\phi$ & preprocessing function for state \\
$Q$ & Q value in reinforcement learning \\
$V$ & V value in reinforcement learning \\
$J$ & objective function \\
\\
$\mu$ & utility of contract(offer), utility of agent, expected profit of a factory \\
$p$ & price \\
$q$ & quantity \\
$m$ & cost of production \\
$P$ & total producible quantity \\
$d$ & step(day) of simulation \\
\end{tabular}

\addsec*{Special identifier}

\begin{tabular}{p{1.5cm} p{12cm}}
$\mS_t$  & state at $t$ \\
$\mA_t$  & action at $t$ \\
$\mR_t$  & reward at $t$ \\
$\mG_t$  & return (cumulative discounted reward) following $t$ \\
$\theta_{k}$ & parameters of old policy, in page 21 \\
$\hat{A}$ & advantage function, used in ppo, 21\\

\\
$\pi(s)$  & action taken in state $s$ under \texttt{deterministic} policy $\pi$ \\
$\pi_{\theta_{k}}$ & old policy in ppo, in page 21 \\
$\pi(a|s)$  & probability of taking action $a$ in state $s$ under stochastic policy $\pi$ \\
$p(s^{\prime},r|s,a)$  & probability of transitioning to state $s'$, with reward $r$, from $s$, $a$ \\
\\
$v_\pi(s)$ & value of state $s$ under policy $\pi$ (except return) \\
$v_*(s)$ & value of state $s$ under the optimal policy \\
$q_{\pi}(s, a)$ & value of taking action $a$ in the state $s$ under policy $\pi$ \\
$q_{*}(s, a)$ & value of taking action $a$ in the state $s$ under optimal policy \\
$\mV_{t}(s)$ & estimate (a random variable) of $v_{\pi}(s)$ or $v_{*}(s)$ \\ 
$\mQ_{t}(s, a)$ & estimate (a random variable) of $q_{\pi}(s, a)$ or $q_{*}(s, a)$ \\
$Q_{tot}$ &  centralised action-value function \\
\\
$\mathbf{w}, \mathbf{w}_{t}$ & vector of (possibly learned) weights \\


\end{tabular}

\addsec*{General quantities}

\begin{tabular}{p{1.5cm} p{12cm}}
$\sS$ & set of nonterminal states \\
$\sA(s)$ & set of actions possible in state $s$ \\
$\sR$ & set of possible rewards \\
$\sC$ & set of contracts(i.e., agreements signed by agent) \\
\end{tabular}