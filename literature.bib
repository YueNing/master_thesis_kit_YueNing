% This file was created with JabRef 2.10.
% Encoding: UTF8

@manual{Kohm2013,
  title = {KOMA-Script -- ein wandelbares LaTeX-2-Paket},
  author = {Markus Kohm and Jens-Uwe Morawski},
  year = {2013},
	url = {http://mirrors.ctan.org/macros/latex/contrib/koma-script/doc/scrguide.pdf},
  langid =  {ngerman}
}

@manual{Cubitt2013,
  title = {The cleveref package},
	author = {Toby Cubitt},
	year = {2013},
	url = {http://mirrors.ctan.org/macros/latex/contrib/cleveref/cleveref.pdf},
  langid = {american}
}

@manual{Fear2005,
  title = {Publication quality tables in LaTeX},
	author = {Simon Fear},
	year = {2005},
	url = {http://mirrors.ctan.org/macros/latex/contrib/booktabs/booktabs.pdf},
  langid = {american}
}

@manual{Cochran2005,
  title = {The Subfig Package},
	author = {Steven Cochran and Vafa Karen-Pahlav},
	year = {2005},
	url = {http://mirrors.ctan.org/macros/latex/contrib/subfig/subfig.pdf},
  langid = {american}
}

@manual{Hoffmann2014,
  title = {The Listings Package},
	author = {Carsten Heinz and  Brooks Moses and  Jobst Hoffmann},
	year = {2014},
	url = {http://mirrors.ctan.org/macros/latex/contrib/listings/listings.pdf},
  langid = {american}
}

@manual{Sommerfeldt2004,
  title = {The rotfloat package},
	author = {Axel Sommerfeldt},
	year = {2004},
	url = {http://mirrors.ctan.org/macros/latex/contrib/rotfloat/rotfloat.pdf},
  langid = {american}
}

@manual{Tantau2013,
  title = {TikZ \& PGF},
	author = {Till Tantau},
	year = {2013},
	url = {http://mirrors.ctan.org/graphics/pgf/base/doc/pgfmanual.pdf},
  langid = {american}
}

@manual{Feuersaenger2014,
  title = {Manual for Package pgfplots},
	author = {Christian Feuersänger},
	year = {2014},
	url = {http://mirrors.ctan.org/graphics/pgf/contrib/pgfplots/doc/pgfplots.pdf},
  langid = {american}
}

@manual{Lehman2013,
  title = {The biblatex Package},
	author = {Philipp Lehman and Philip Kime and Audrey Boruvka and Joseph Wright},
	year = {2013},
	url = {http://mirrors.ctan.org/macros/latex/contrib/biblatex/doc/biblatex.pdf},
  langid = {american}
}

@manual{May2011,
  title = {An Extension of the LaTeX-Theorem Environment},
	author = {Wolfgang May and Andreas Schedler},
	year = {2011},
	url = {http://mirrors.ctan.org/macros/latex/contrib/ntheorem/ntheorem.pdf},
  langid = {american}
}


@manual{ams1999a,
  title = {User’s Guide for the amsmath Package},
	author = {{American Mathematical Society}},
	year = {1999},
	url = {http://mirrors.ctan.org/macros/latex/required/amslatex/math/amsldoc.pdf},
  langid = {american}
}

@manual{ams1999b,
  title = {Sample Paper for the amsmath Package},
	author = {{American Mathematical Society}},
	year = {1999},
	url = {http://mirrors.ctan.org/macros/latex/required/amslatex/math/testmath.pdf},
  langid = {american}
}

@manual{talbot2014,
  title = {User Manual for glossaries.sty v4.09},
	author = {Nicola Talbot},
	year = {2014},
	url = {http://mirrors.ctan.org/macros/latex/contrib/glossaries/glossaries-user.pdf},
  langid = {american}
}

@Inbook{Kreps1989,
author="Kreps, David M.",
editor="Eatwell, John
and Milgate, Murray
and Newman, Peter",
title="Nash Equilibrium",
bookTitle="Game Theory",
year="1989",
publisher="Palgrave Macmillan UK",
address="London",
pages="167--177",
abstract="The concept of a Nash equilibrium plays a central role in noncooperative game theory. Due in its current formalization to John Nash (1950, 1951), it goes back at least to Cournot (1838). This entry begins with the formal definition of a Nash equilibrium and with some of the mathematical properties of equilibria. Then we ask: To what question is `Nash equilibrium' the answer? The answer that we suggest motivates further questions of equilibrium selection, which we consider in two veins: the informal notions, such as Schelling's (1960) focal points; and the formal theories for refining or perfecting Nash equilibria, due largely to Selten (1965, 1975). We conclude with a brief discussion of two related issues: Harsanyi's (1967--8) notion of a game of incomplete information and Aumann's (1973) correlated equilibria.",
isbn="978-1-349-20181-5",
doi="10.1007/978-1-349-20181-5_19",
url="https://doi.org/10.1007/978-1-349-20181-5_19"
}

@Inbook{Aydoğan2017,
author="Aydo{\u{g}}an, Reyhan
and Festen, David
and Hindriks, Koen V.
and Jonker, Catholijn M.",
editor="Fujita, Katsuhide
and Bai, Quan
and Ito, Takayuki
and Zhang, Minjie
and Ren, Fenghui
and Aydo{\u{g}}an, Reyhan
and Hadfi, Rafik",
title="Alternating Offers Protocols for Multilateral Negotiation",
bookTitle="Modern Approaches to Agent-based Complex Automated Negotiation",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="153--167",
abstract="This paper presents a general framework for multilateral turn-taking protocols and two fully specified protocols namely Stacked Alternating Offers Protocol (SAOP) and Alternating Multiple Offers Protocol (AMOP). In SAOP, agents can make a bid, accept the most recent bid or walk way (i.e., end the negotiation without an agreement) when it is their turn. AMOP has two different phases: bidding and voting. The agents make their bid in the bidding phase and vote the underlying bids in the voting phase. Unlike SAOP, AMOP does not support walking away option. In both protocols, negotiation ends when the negotiating agents reach a joint agreement or some deadline criterion applies. The protocols have been evaluated empirically, showing that SAOP outperforms AMOP with the same type of conceder agents in a time-based deadline setting. SAOP was used in the ANAC 2015 competition for automated negotiating agents.",
isbn="978-3-319-51563-2",
doi="10.1007/978-3-319-51563-2_10",
url="https://doi.org/10.1007/978-3-319-51563-2_10"
}

@article{Lin2014,
author = {Lin, Raz and Kraus, Sarit and Baarslag, Tim and Tykhonov, Dmytro and Hindriks, Koen and Jonker, Catholijn},
year = {2014},
month = {02},
pages = {48-70},
title = {Genius: An Integrated Environment for Supporting the Design of Generic Automated Negotiators},
volume = {30},
journal = {Computational Intelligence},
doi = {10.1111/j.1467-8640.2012.00463.x}
}

@inbook{Mohammad2019,
author = {Mohammad, Yasser and Viqueira, Enrique and Ayerza, Nahum and Greenwald, Amy and Nakadai, Shinji and Morinaga, Satoshi},
year = {2019},
month = {10},
pages = {153-169},
title = {Supply Chain Management World},
isbn = {978-3-030-33791-9},
doi = {10.1007/978-3-030-33792-6_10}
}

@misc{bagga2020deep,
      title={A Deep Reinforcement Learning Approach to Concurrent Bilateral Negotiation}, 
      author={Pallavi Bagga and Nicola Paoletti and Bedour Alrayes and Kostas Stathis},
      year={2020},
      eprint={2001.11785},
      archivePrefix={arXiv},
      primaryClass={cs.MA}
}

@inproceedings{Bakker2019RLBOAAM,
  title={RLBOA: A Modular Reinforcement Learning Framework for Autonomous Negotiating Agents},
  author={Jasper Bakker and Aron Hammond and D. Bloembergen and T. Baarslag},
  booktitle={AAMAS},
  year={2019}
}

@ARTICLE{L2017NegoSI,
  author={L. {Zhou} and P. {Yang} and C. {Chen} and Y. {Gao}},
  journal={IEEE Transactions on Cybernetics}, 
  title={Multiagent Reinforcement Learning With Sparse Interactions by Negotiation and Knowledge Transfer}, 
  year={2017},
  volume={47},
  number={5},
  pages={1238-1250},
  abstract={Reinforcement learning has significant applications for multiagent systems, especially in unknown dynamic environments. However, most multiagent reinforcement learning (MARL) algorithms suffer from such problems as exponential computation complexity in the joint state-action space, which makes it difficult to scale up to realistic multiagent problems. In this paper, a novel algorithm named negotiation-based MARL with sparse interactions (NegoSIs) is presented. In contrast to traditional sparse-interaction-based MARL algorithms, NegoSI adopts the equilibrium concept and makes it possible for agents to select the nonstrict equilibrium-dominating strategy profile (nonstrict EDSP) or meta equilibrium for their joint actions. The presented NegoSI algorithm consists of four parts: 1) the equilibrium-based framework for sparse interactions; 2) the negotiation for the equilibrium set; 3) the minimum variance method for selecting one joint action; and 4) the knowledge transfer of local ${Q}$ -values. In this integrated algorithm, three techniques, i.e., unshared value functions, equilibrium solutions, and sparse interactions are adopted to achieve privacy protection, better coordination and lower computational complexity, respectively. To evaluate the performance of the presented NegoSI algorithm, two groups of experiments are carried out regarding three criteria: 1) steps of each episode; 2) rewards of each episode; and 3) average runtime. The first group of experiments is conducted using six grid world games and shows fast convergence and high scalability of the presented algorithm. Then in the second group of experiments NegoSI is applied to an intelligent warehouse problem and simulated results demonstrate the effectiveness of the presented NegoSI algorithm compared with other state-of-the-art MARL algorithms.},
  keywords={Games;Heuristic algorithms;Markov processes;Robot kinematics;Knowledge transfer;Modeling;Knowledge transfer;multiagent reinforcement learning (MARL);negotiation;sparse interactions},
  doi={10.1109/TCYB.2016.2543238},
  ISSN={2168-2275},
  month={05},}

@inproceedings{Williams12Concurrent,
author = {Williams, Colin and Robu, Valentin and Gerding, Enrico and Jennings, Nicholas},
year = {2012},
month = {05},
pages = {},
title = {Negotiating Concurrently with Unknown Opponents in Complex, Real-Time Domains}
}

@misc{brockman2016openai,
      title={OpenAI Gym}, 
      author={Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
      year={2016},
      eprint={1606.01540},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{NEURIPS2019_bdbca288,
 author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
 url = {https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},
 volume = {32},
 year = {2019}
}

@misc{stable-baselines,
  author = {Hill, Ashley and Raffin, Antonin and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Traore, Rene and Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai},
  title = {Stable Baselines},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/hill-a/stable-baselines}},
}

@article{Rubinstein1982,
author = {Rubinstein, Ariel},
year = {1982},
month = {02},
pages = {97-109},
title = {Perfect Equilibrium in A Bargaining Model},
volume = {50},
journal = {Econometrica},
doi = {10.2307/1912531}
}

@manual{Suman2020,
  title = {Is machine learning required for deep learning?},
	author = {SUMAN},
	year = {2020},
	url = {https://ai.stackexchange.com/questions/15859/is-machine-learning-required-for-deep-learning},
  langid = {american}
}

@manual{Sutton2018,
  title = {Reinforcement Learning: An Introduction, Second Edition},
	author = {Richard S. Sutton and Andrew G. Barto},
	year = {2018},
	publisher = {MIT Press Cambridge MA},
	url ={https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf},
  langid = {american}
}

@Article{en13010123,
AUTHOR = {Fang, Xiaohan and Wang, Jinkuan and Song, Guanru and Han, Yinghua and Zhao, Qiang and Cao, Zhiao},
TITLE = {Multi-Agent Reinforcement Learning Approach for Residential Microgrid Energy Scheduling},
JOURNAL = {Energies},
VOLUME = {13},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {123},
URL = {https://www.mdpi.com/1996-1073/13/1/123},
ISSN = {1996-1073},
ABSTRACT = {Residential microgrid is widely considered as a new paradigm of the home energy management system. The complexity of Microgrid Energy Scheduling (MES) is increasing with the integration of Electric Vehicles (EVs) and Renewable Generations (RGs). Moreover, it is challenging to determine optimal scheduling strategies to guarantee the efficiency of the microgrid market and to balance all market participants’ benefits. In this paper, a Multi-Agent Reinforcement Learning (MARL) approach for residential MES is proposed to promote the autonomy and fairness of microgrid market operation. First, a multi-agent based residential microgrid model including Vehicle-to-Grid (V2G) and RGs is constructed and an auction-based microgrid market is built. Then, distinguish from Single-Agent Reinforcement Learning (SARL), MARL can achieve distributed autonomous learning for each agent and realize the equilibrium of all agents’ benefits, therefore, we formulate an equilibrium-based MARL framework according to each participant’ market orientation. Finally, to guarantee the fairness and privacy of the MARL process, we proposed an improved optimal Equilibrium Selection-MARL (ES-MARL) algorithm based on two mechanisms, private negotiation and maximum average reward. Simulation results demonstrate the overall performance and efficiency of proposed MARL are superior to that of SARL. Besides, it is verified that the improved ES-MARL can get higher average profit to balance all agents.},
DOI = {10.3390/en13010123}
}

@article{Rashid2018,
author = {Rashid, Tabish and Samvelyan, Mikayel and Witt, Christian and Farquhar, Gregory and Foerster, Jakob and Whiteson, Shimon},
year = {2018},
month = {03},
pages = {},
title = {QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning}
}
